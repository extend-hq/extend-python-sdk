# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.max_page_size import MaxPageSize
from ..types.next_page_token import NextPageToken
from ..types.sort_by import SortBy
from ..types.sort_dir import SortDir
from .raw_client import AsyncRawEvaluationSetsClient, RawEvaluationSetsClient
from .types.evaluation_sets_create_response import EvaluationSetsCreateResponse
from .types.evaluation_sets_list_response import EvaluationSetsListResponse
from .types.evaluation_sets_retrieve_response import EvaluationSetsRetrieveResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluationSetsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawEvaluationSetsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawEvaluationSetsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawEvaluationSetsClient
        """
        return self._raw_client

    def list(
        self,
        *,
        entity_id: typing.Optional[str] = None,
        sort_by: typing.Optional[SortBy] = None,
        sort_dir: typing.Optional[SortDir] = None,
        next_page_token: typing.Optional[NextPageToken] = None,
        max_page_size: typing.Optional[MaxPageSize] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetsListResponse:
        """
        List evaluation sets in your account.

        Parameters
        ----------
        entity_id : typing.Optional[str]
            The ID of the extractor, classifier, or splitter to filter evaluation sets by.

            Example: `"extractor_Xj8mK2pL9nR4vT7qY5wZ"`

        sort_by : typing.Optional[SortBy]

        sort_dir : typing.Optional[SortDir]

        next_page_token : typing.Optional[NextPageToken]

        max_page_size : typing.Optional[MaxPageSize]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetsListResponse
            Successfully retrieved evaluation sets

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_sets.list(
            entity_id="entity_id_here",
            next_page_token="xK9mLPqRtN3vS8wF5hB2cQ==:zWvUxYjM4nKpL7aDgE9HbTcR2mAyX3/Q+CNkfBSw1dZ=",
        )
        """
        _response = self._raw_client.list(
            entity_id=entity_id,
            sort_by=sort_by,
            sort_dir=sort_dir,
            next_page_token=next_page_token,
            max_page_size=max_page_size,
            request_options=request_options,
        )
        return _response.data

    def create(
        self,
        *,
        name: str,
        entity_id: str,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetsCreateResponse:
        """
        Evaluation sets are collections of files and expected outputs that are used to evaluate the performance of a given extractor, classifier, or splitter. This endpoint will create a new evaluation set, which items can be added to using the [Create Evaluation Set Item](https://docs.extend.ai/2026-01-01/developers/api-reference/endpoints/evaluation/create-evaluation-set-item) endpoint.

        Note: It is not necessary to create an evaluation set via API. You can also create an evaluation set via the Extend dashboard and take the ID from there. To learn more about how to create evaluation sets, see the [Evaluation Sets](https://docs.extend.ai/product/evaluation/overview) product page.

        Parameters
        ----------
        name : str
            The name of the evaluation set.

            Example: `"Invoice Processing Test Set"`

        entity_id : str
            The ID of the extractor, classifier, or splitter to create an evaluation set for. Evaluation sets can in theory be run against any extractor, classifier, or splitter, but it is required to associate the evaluation set with a primary extractor, classifier, or splitter.

            Example: `"extractor_Xj8mK2pL9nR4vT7qY5wZ"`

        description : typing.Optional[str]
            A description of what this evaluation set is used for.

            Example: `"Q4 2023 vendor invoices"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetsCreateResponse
            Successfully created evaluation set

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_sets.create(
            name="My Evaluation Set",
            entity_id="entity_id_here",
        )
        """
        _response = self._raw_client.create(
            name=name, entity_id=entity_id, description=description, request_options=request_options
        )
        return _response.data

    def retrieve(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationSetsRetrieveResponse:
        """
        Retrieve a specific evaluation set by ID. This returns an evaluation set object, but does not include the items in the evaluation set. You can use the [List Evaluation Set Items](https://docs.extend.ai/2026-01-01/developers/api-reference/endpoints/evaluation/list-evaluation-set-items) endpoint to get the items in an evaluation set.

        Parameters
        ----------
        id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetsRetrieveResponse
            Successfully retrieved evaluation set

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_sets.retrieve(
            id="evaluation_set_id_here",
        )
        """
        _response = self._raw_client.retrieve(id, request_options=request_options)
        return _response.data


class AsyncEvaluationSetsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawEvaluationSetsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawEvaluationSetsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawEvaluationSetsClient
        """
        return self._raw_client

    async def list(
        self,
        *,
        entity_id: typing.Optional[str] = None,
        sort_by: typing.Optional[SortBy] = None,
        sort_dir: typing.Optional[SortDir] = None,
        next_page_token: typing.Optional[NextPageToken] = None,
        max_page_size: typing.Optional[MaxPageSize] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetsListResponse:
        """
        List evaluation sets in your account.

        Parameters
        ----------
        entity_id : typing.Optional[str]
            The ID of the extractor, classifier, or splitter to filter evaluation sets by.

            Example: `"extractor_Xj8mK2pL9nR4vT7qY5wZ"`

        sort_by : typing.Optional[SortBy]

        sort_dir : typing.Optional[SortDir]

        next_page_token : typing.Optional[NextPageToken]

        max_page_size : typing.Optional[MaxPageSize]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetsListResponse
            Successfully retrieved evaluation sets

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_sets.list(
                entity_id="entity_id_here",
                next_page_token="xK9mLPqRtN3vS8wF5hB2cQ==:zWvUxYjM4nKpL7aDgE9HbTcR2mAyX3/Q+CNkfBSw1dZ=",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list(
            entity_id=entity_id,
            sort_by=sort_by,
            sort_dir=sort_dir,
            next_page_token=next_page_token,
            max_page_size=max_page_size,
            request_options=request_options,
        )
        return _response.data

    async def create(
        self,
        *,
        name: str,
        entity_id: str,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetsCreateResponse:
        """
        Evaluation sets are collections of files and expected outputs that are used to evaluate the performance of a given extractor, classifier, or splitter. This endpoint will create a new evaluation set, which items can be added to using the [Create Evaluation Set Item](https://docs.extend.ai/2026-01-01/developers/api-reference/endpoints/evaluation/create-evaluation-set-item) endpoint.

        Note: It is not necessary to create an evaluation set via API. You can also create an evaluation set via the Extend dashboard and take the ID from there. To learn more about how to create evaluation sets, see the [Evaluation Sets](https://docs.extend.ai/product/evaluation/overview) product page.

        Parameters
        ----------
        name : str
            The name of the evaluation set.

            Example: `"Invoice Processing Test Set"`

        entity_id : str
            The ID of the extractor, classifier, or splitter to create an evaluation set for. Evaluation sets can in theory be run against any extractor, classifier, or splitter, but it is required to associate the evaluation set with a primary extractor, classifier, or splitter.

            Example: `"extractor_Xj8mK2pL9nR4vT7qY5wZ"`

        description : typing.Optional[str]
            A description of what this evaluation set is used for.

            Example: `"Q4 2023 vendor invoices"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetsCreateResponse
            Successfully created evaluation set

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_sets.create(
                name="My Evaluation Set",
                entity_id="entity_id_here",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create(
            name=name, entity_id=entity_id, description=description, request_options=request_options
        )
        return _response.data

    async def retrieve(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationSetsRetrieveResponse:
        """
        Retrieve a specific evaluation set by ID. This returns an evaluation set object, but does not include the items in the evaluation set. You can use the [List Evaluation Set Items](https://docs.extend.ai/2026-01-01/developers/api-reference/endpoints/evaluation/list-evaluation-set-items) endpoint to get the items in an evaluation set.

        Parameters
        ----------
        id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetsRetrieveResponse
            Successfully retrieved evaluation set

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_sets.retrieve(
                id="evaluation_set_id_here",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.retrieve(id, request_options=request_options)
        return _response.data
