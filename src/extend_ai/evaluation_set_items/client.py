# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..requests.provided_processor_output import ProvidedProcessorOutputParams
from ..types.evaluation_set_item import EvaluationSetItem
from ..types.max_page_size import MaxPageSize
from ..types.next_page_token import NextPageToken
from ..types.sort_by import SortBy
from ..types.sort_dir import SortDir
from .raw_client import AsyncRawEvaluationSetItemsClient, RawEvaluationSetItemsClient
from .requests.evaluation_set_items_create_request_items_item import EvaluationSetItemsCreateRequestItemsItemParams
from .types.evaluation_set_items_create_response import EvaluationSetItemsCreateResponse
from .types.evaluation_set_items_delete_response import EvaluationSetItemsDeleteResponse
from .types.evaluation_set_items_list_response import EvaluationSetItemsListResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluationSetItemsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawEvaluationSetItemsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawEvaluationSetItemsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawEvaluationSetItemsClient
        """
        return self._raw_client

    def list(
        self,
        evaluation_set_id: str,
        *,
        sort_by: typing.Optional[SortBy] = None,
        sort_dir: typing.Optional[SortDir] = None,
        next_page_token: typing.Optional[NextPageToken] = None,
        max_page_size: typing.Optional[MaxPageSize] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemsListResponse:
        """
        List items in a specific evaluation set.

        Returns a summary of each evaluation set item. Use the [Get Evaluation Set Item](https://docs.extend.ai/2026-02-09/developers/api-reference/endpoints/evaluation/get-evaluation-set-item) endpoint to get the full details of an evaluation set item.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        sort_by : typing.Optional[SortBy]

        sort_dir : typing.Optional[SortDir]

        next_page_token : typing.Optional[NextPageToken]

        max_page_size : typing.Optional[MaxPageSize]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemsListResponse
            List of evaluation set items

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_set_items.list(
            evaluation_set_id="evaluation_set_id_here",
            next_page_token="xK9mLPqRtN3vS8wF5hB2cQ==:zWvUxYjM4nKpL7aDgE9HbTcR2mAyX3/Q+CNkfBSw1dZ=",
        )
        """
        _response = self._raw_client.list(
            evaluation_set_id,
            sort_by=sort_by,
            sort_dir=sort_dir,
            next_page_token=next_page_token,
            max_page_size=max_page_size,
            request_options=request_options,
        )
        return _response.data

    def create(
        self,
        evaluation_set_id: str,
        *,
        items: typing.Sequence[EvaluationSetItemsCreateRequestItemsItemParams],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemsCreateResponse:
        """
        Evaluation set items are the individual files and expected outputs that are used to evaluate the performance of a given extractor, classifier, or splitter in Extend. This endpoint will create new evaluation set items in Extend, which will be used during an evaluation run.

        **Limit:** You can create up to 100 items at a time.

        Learn more about how to create evaluation set items in the [Evaluation Sets](https://docs.extend.ai/product/evaluation/overview) product page.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        items : typing.Sequence[EvaluationSetItemsCreateRequestItemsItemParams]
            An array of objects representing the evaluation set items to create.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemsCreateResponse
            Evaluation set items created successfully

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_set_items.create(
            evaluation_set_id="evaluation_set_id_here",
            items=[{"file_id": "file_id_here", "expected_output": {}}],
        )
        """
        _response = self._raw_client.create(evaluation_set_id, items=items, request_options=request_options)
        return _response.data

    def retrieve(
        self, evaluation_set_id: str, item_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationSetItem:
        """
        Get details of an evaluation set item.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        item_id : str
            The ID of the evaluation set item.

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItem
            Evaluation set item details

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_set_items.retrieve(
            evaluation_set_id="evaluation_set_id_here",
            item_id="evaluation_set_item_id_here",
        )
        """
        _response = self._raw_client.retrieve(evaluation_set_id, item_id, request_options=request_options)
        return _response.data

    def update(
        self,
        evaluation_set_id: str,
        item_id: str,
        *,
        expected_output: ProvidedProcessorOutputParams,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItem:
        """
        If you need to change the expected output for a given evaluation set item, you can use this endpoint to update the item. This can be useful if you need to correct an error in the expected output or if the output of the extractor, classifier, or splitter has changed.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        item_id : str
            The ID of the evaluation set item.

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        expected_output : ProvidedProcessorOutputParams
            The expected output of the extractor, classifier, or splitter when run against the file. This must conform to the output schema of the entity associated with the evaluation set.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItem
            Successfully updated evaluation set item

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_set_items.update(
            evaluation_set_id="evaluation_set_id_here",
            item_id="evaluation_set_item_id_here",
            expected_output={},
        )
        """
        _response = self._raw_client.update(
            evaluation_set_id, item_id, expected_output=expected_output, request_options=request_options
        )
        return _response.data

    def delete(
        self, evaluation_set_id: str, item_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationSetItemsDeleteResponse:
        """
        Delete an evaluation set item.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        item_id : str
            The ID of the evaluation set item.

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemsDeleteResponse
            Evaluation set item deleted successfully

        Examples
        --------
        from extend_ai import Extend

        client = Extend(
            token="YOUR_TOKEN",
        )
        client.evaluation_set_items.delete(
            evaluation_set_id="evaluation_set_id_here",
            item_id="evaluation_set_item_id_here",
        )
        """
        _response = self._raw_client.delete(evaluation_set_id, item_id, request_options=request_options)
        return _response.data


class AsyncEvaluationSetItemsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawEvaluationSetItemsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawEvaluationSetItemsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawEvaluationSetItemsClient
        """
        return self._raw_client

    async def list(
        self,
        evaluation_set_id: str,
        *,
        sort_by: typing.Optional[SortBy] = None,
        sort_dir: typing.Optional[SortDir] = None,
        next_page_token: typing.Optional[NextPageToken] = None,
        max_page_size: typing.Optional[MaxPageSize] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemsListResponse:
        """
        List items in a specific evaluation set.

        Returns a summary of each evaluation set item. Use the [Get Evaluation Set Item](https://docs.extend.ai/2026-02-09/developers/api-reference/endpoints/evaluation/get-evaluation-set-item) endpoint to get the full details of an evaluation set item.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        sort_by : typing.Optional[SortBy]

        sort_dir : typing.Optional[SortDir]

        next_page_token : typing.Optional[NextPageToken]

        max_page_size : typing.Optional[MaxPageSize]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemsListResponse
            List of evaluation set items

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_items.list(
                evaluation_set_id="evaluation_set_id_here",
                next_page_token="xK9mLPqRtN3vS8wF5hB2cQ==:zWvUxYjM4nKpL7aDgE9HbTcR2mAyX3/Q+CNkfBSw1dZ=",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list(
            evaluation_set_id,
            sort_by=sort_by,
            sort_dir=sort_dir,
            next_page_token=next_page_token,
            max_page_size=max_page_size,
            request_options=request_options,
        )
        return _response.data

    async def create(
        self,
        evaluation_set_id: str,
        *,
        items: typing.Sequence[EvaluationSetItemsCreateRequestItemsItemParams],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItemsCreateResponse:
        """
        Evaluation set items are the individual files and expected outputs that are used to evaluate the performance of a given extractor, classifier, or splitter in Extend. This endpoint will create new evaluation set items in Extend, which will be used during an evaluation run.

        **Limit:** You can create up to 100 items at a time.

        Learn more about how to create evaluation set items in the [Evaluation Sets](https://docs.extend.ai/product/evaluation/overview) product page.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        items : typing.Sequence[EvaluationSetItemsCreateRequestItemsItemParams]
            An array of objects representing the evaluation set items to create.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemsCreateResponse
            Evaluation set items created successfully

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_items.create(
                evaluation_set_id="evaluation_set_id_here",
                items=[{"file_id": "file_id_here", "expected_output": {}}],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create(evaluation_set_id, items=items, request_options=request_options)
        return _response.data

    async def retrieve(
        self, evaluation_set_id: str, item_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationSetItem:
        """
        Get details of an evaluation set item.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        item_id : str
            The ID of the evaluation set item.

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItem
            Evaluation set item details

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_items.retrieve(
                evaluation_set_id="evaluation_set_id_here",
                item_id="evaluation_set_item_id_here",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.retrieve(evaluation_set_id, item_id, request_options=request_options)
        return _response.data

    async def update(
        self,
        evaluation_set_id: str,
        item_id: str,
        *,
        expected_output: ProvidedProcessorOutputParams,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationSetItem:
        """
        If you need to change the expected output for a given evaluation set item, you can use this endpoint to update the item. This can be useful if you need to correct an error in the expected output or if the output of the extractor, classifier, or splitter has changed.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        item_id : str
            The ID of the evaluation set item.

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        expected_output : ProvidedProcessorOutputParams
            The expected output of the extractor, classifier, or splitter when run against the file. This must conform to the output schema of the entity associated with the evaluation set.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItem
            Successfully updated evaluation set item

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_items.update(
                evaluation_set_id="evaluation_set_id_here",
                item_id="evaluation_set_item_id_here",
                expected_output={},
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.update(
            evaluation_set_id, item_id, expected_output=expected_output, request_options=request_options
        )
        return _response.data

    async def delete(
        self, evaluation_set_id: str, item_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationSetItemsDeleteResponse:
        """
        Delete an evaluation set item.

        Parameters
        ----------
        evaluation_set_id : str
            The ID of the evaluation set.

            Example: `"ev_2LcgeY_mp2T5yPaEuq5Lw"`

        item_id : str
            The ID of the evaluation set item.

            Example: `"evi_kR9mNP12Qw4yTv8BdR3H"`

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationSetItemsDeleteResponse
            Evaluation set item deleted successfully

        Examples
        --------
        import asyncio

        from extend_ai import AsyncExtend

        client = AsyncExtend(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.evaluation_set_items.delete(
                evaluation_set_id="evaluation_set_id_here",
                item_id="evaluation_set_item_id_here",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete(evaluation_set_id, item_id, request_options=request_options)
        return _response.data
